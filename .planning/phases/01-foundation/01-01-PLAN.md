---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Dockerfile
  - docker-compose.yml
  - requirements.txt
  - app/__init__.py
  - app/main.py
autonomous: false

must_haves:
  truths:
    - "User can start system with docker-compose up"
    - "Container has working GPU access"
    - "GET /health returns 200 with GPU info"
    - "Container refuses to start without GPU"
  artifacts:
    - path: "Dockerfile"
      provides: "CUDA-enabled Python container"
      contains: "nvidia/cuda:11.8"
    - path: "docker-compose.yml"
      provides: "Single-command startup"
      contains: "deploy:"
    - path: "requirements.txt"
      provides: "Python dependencies"
      contains: "fastapi"
    - path: "app/main.py"
      provides: "FastAPI app with lifespan and health endpoint"
      exports: ["app"]
  key_links:
    - from: "docker-compose.yml"
      to: "Dockerfile"
      via: "build context"
      pattern: "build:\\s*\\."
    - from: "app/main.py"
      to: "pynvml"
      via: "GPU monitoring in health endpoint"
      pattern: "nvml"
    - from: "docker-compose.yml"
      to: "NVIDIA GPU"
      via: "device reservation"
      pattern: "capabilities.*gpu"
---

<objective>
Create GPU-enabled Docker environment with FastAPI health endpoint that validates GPU access at startup.

Purpose: Establish the foundational infrastructure for the 3D reconstruction pipeline. Without working GPU access and a running API server, no subsequent phases can proceed.

Output: Docker container that starts with `docker-compose up`, validates GPU at startup, and exposes `/health` endpoint with GPU metrics.
</objective>

<execution_context>
@/home/devuser/.claude/get-shit-done/workflows/execute-plan.md
@/home/devuser/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Docker infrastructure</name>
  <files>
    - Dockerfile
    - docker-compose.yml
    - requirements.txt
  </files>
  <action>
Create three files for the Docker infrastructure:

**Dockerfile:**
- Base image: `nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04`
- Install Python 3.10 via apt (python3.10, python3.10-dev, python3-pip)
- Set Python 3.10 as default using update-alternatives
- Set WORKDIR to /app
- Copy and install requirements.txt with --no-cache-dir
- Copy app/ directory
- EXPOSE 8000
- Use exec form CMD: `["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]`

**docker-compose.yml:**
- Version 3.8
- Single service named `api`
- Build from current directory (build: .)
- Port mapping 8000:8000
- Volume mount ./app:/app/app for development
- Environment: NVIDIA_VISIBLE_DEVICES=0, LOG_LEVEL=INFO
- GPU config using modern syntax:
  ```yaml
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
  ```
- Healthcheck: curl -f http://localhost:8000/health (interval 30s, timeout 10s, retries 3, start_period 40s)

**requirements.txt:**
```
fastapi==0.115.8
uvicorn[standard]==0.34.0
pydantic>=2.0
nvidia-ml-py
```

Do NOT use shell form CMD in Dockerfile (breaks signal forwarding and lifespan events).
  </action>
  <verify>
Files exist and have correct content:
- `cat Dockerfile` shows CUDA base image and exec form CMD
- `cat docker-compose.yml` shows GPU reservation with capabilities
- `cat requirements.txt` shows all 4 dependencies
  </verify>
  <done>
Three files created: Dockerfile with CUDA 11.8 base, docker-compose.yml with GPU reservation, requirements.txt with FastAPI and nvidia-ml-py
  </done>
</task>

<task type="auto">
  <name>Task 2: Create FastAPI application with GPU validation</name>
  <files>
    - app/__init__.py
    - app/main.py
  </files>
  <action>
Create the FastAPI application with fail-fast GPU validation:

**app/__init__.py:**
Empty file (package marker)

**app/main.py:**
```python
"""3D Reconstruction API - Foundation"""
from contextlib import asynccontextmanager
import logging

from fastapi import FastAPI
from fastapi.responses import JSONResponse
from pynvml import (
    nvmlInit,
    nvmlShutdown,
    nvmlDeviceGetHandleByIndex,
    nvmlDeviceGetName,
    nvmlDeviceGetMemoryInfo,
    nvmlSystemGetDriverVersion,
)

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

MINIMUM_VRAM_GB = 12

# Shared state for GPU info (populated at startup)
gpu_state: dict = {}


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Validate GPU at startup, cleanup on shutdown."""
    logger.info("Starting GPU validation...")

    try:
        nvmlInit()
        handle = nvmlDeviceGetHandleByIndex(0)
        memory = nvmlDeviceGetMemoryInfo(handle)

        total_gb = memory.total / (1024**3)
        if total_gb < MINIMUM_VRAM_GB:
            raise RuntimeError(
                f"Insufficient VRAM: {total_gb:.1f}GB < {MINIMUM_VRAM_GB}GB required"
            )

        # Cache GPU info for health checks
        gpu_state["name"] = nvmlDeviceGetName(handle)
        gpu_state["driver_version"] = nvmlSystemGetDriverVersion()
        gpu_state["memory_total_gb"] = round(total_gb, 2)
        gpu_state["initialized"] = True

        logger.info(f"GPU validated: {gpu_state['name']} with {total_gb:.1f}GB VRAM")
        nvmlShutdown()

    except Exception as e:
        logger.error(f"GPU validation failed: {e}")
        raise RuntimeError(f"GPU unavailable or insufficient: {e}")

    yield  # Application runs

    # Shutdown cleanup
    gpu_state.clear()
    logger.info("Shutdown complete")


app = FastAPI(
    title="3D Reconstruction API",
    version="0.1.0",
    lifespan=lifespan
)


@app.get("/health")
async def health_check():
    """Return health status with GPU information."""
    if not gpu_state.get("initialized"):
        return JSONResponse(
            status_code=503,
            content={"status": "unhealthy", "error": "GPU not initialized"}
        )

    try:
        nvmlInit()
        handle = nvmlDeviceGetHandleByIndex(0)
        memory = nvmlDeviceGetMemoryInfo(handle)
        nvmlShutdown()

        return {
            "status": "healthy",
            "gpu": {
                "name": gpu_state["name"],
                "driver_version": gpu_state["driver_version"],
                "memory_total_gb": gpu_state["memory_total_gb"],
                "memory_free_gb": round(memory.free / (1024**3), 2),
                "memory_used_gb": round(memory.used / (1024**3), 2),
            }
        }
    except Exception as e:
        return JSONResponse(
            status_code=503,
            content={"status": "unhealthy", "error": str(e)}
        )
```

Key implementation details:
- Use lifespan context manager (NOT deprecated @app.on_event)
- Fail fast if GPU unavailable or VRAM < 12GB
- Cache GPU name/driver at startup (doesn't change)
- Query live memory stats in health endpoint
- Return 503 on GPU errors (not 500)
  </action>
  <verify>
- `ls app/` shows __init__.py and main.py
- `grep -l "lifespan" app/main.py` finds lifespan usage
- `grep -l "MINIMUM_VRAM_GB" app/main.py` finds VRAM check
- `grep -l "@app.get" app/main.py` finds health endpoint
  </verify>
  <done>
FastAPI app created with lifespan-based GPU validation at startup and /health endpoint returning GPU metrics
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
GPU-enabled Docker environment with FastAPI health endpoint. The container validates GPU access at startup and refuses to start without sufficient VRAM.
  </what-built>
  <how-to-verify>
1. Build and start the container:
   ```bash
   docker-compose up --build
   ```

2. Verify startup logs show GPU validation:
   - Look for: "GPU validated: [GPU name] with XX.XGB VRAM"
   - Container should NOT exit after starting

3. Test health endpoint (in another terminal):
   ```bash
   curl http://localhost:8000/health
   ```
   Expected response (200 OK):
   ```json
   {
     "status": "healthy",
     "gpu": {
       "name": "NVIDIA GeForce RTX 3090",
       "driver_version": "...",
       "memory_total_gb": 24.0,
       "memory_free_gb": XX.X,
       "memory_used_gb": X.X
     }
   }
   ```

4. Verify API docs accessible:
   - Visit http://localhost:8000/docs in browser
   - Should show Swagger UI with /health endpoint

5. Stop with Ctrl+C - verify clean shutdown log appears
  </how-to-verify>
  <resume-signal>Type "approved" if all checks pass, or describe what failed</resume-signal>
</task>

</tasks>

<verification>
Phase 1 requirements verification:

- [ ] DEPLOY-01: Docker container with GPU support (nvidia-docker)
  - Dockerfile uses nvidia/cuda base image
  - docker-compose.yml has GPU reservation

- [ ] DEPLOY-02: Single command startup (docker-compose up)
  - docker-compose.yml configured correctly
  - Container builds and starts

- [ ] API-05: Health endpoint (GET /health)
  - Returns 200 with GPU info when healthy
  - Returns 503 when GPU unavailable
</verification>

<success_criteria>
1. `docker-compose up --build` starts container without errors
2. Container logs show "GPU validated" message with GPU name
3. `curl localhost:8000/health` returns JSON with status "healthy" and GPU info
4. Container refuses to start if no GPU available (RuntimeError at startup)
5. Memory stats in health response show realistic values for RTX 3090 (~24GB total)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
