---
phase: 03-model-integration
plan: 04
type: execute
wave: 3
depends_on: ["03-01", "03-02", "03-03"]
files_modified:
  - app/tasks/reconstruction.py
  - app/api/schemas.py
  - app/api/jobs.py
autonomous: false

must_haves:
  truths:
    - "User can submit job with model_type parameter (reconviagen, nvdiffrec, both)"
    - "Job correctly invokes selected model(s) and produces mesh output"
    - "Sequential model execution (both) cleans VRAM between models"
    - "Job fails gracefully on OOM with user-friendly error"
    - "Output files exist in correct directory structure (reconviagen/, nvdiffrec/)"
  artifacts:
    - path: "app/tasks/reconstruction.py"
      provides: "Real model execution replacing placeholder"
      contains: "from app.models import get_model"
      min_lines: 120
    - path: "app/api/schemas.py"
      provides: "Updated schema with model_type enum"
      contains: "ModelType"
  key_links:
    - from: "app/tasks/reconstruction.py"
      to: "app/models"
      via: "model factory import"
      pattern: "from app\\.models import get_model"
    - from: "app/tasks/reconstruction.py"
      to: "app/services/vram_manager.py"
      via: "VRAM cleanup between models"
      pattern: "cleanup_gpu_memory"
    - from: "app/api/jobs.py"
      to: "app/tasks/reconstruction.py"
      via: "task dispatch with model_type"
      pattern: "process_reconstruction\\.apply_async.*model_type"
---

<objective>
Integrate model wrappers into the reconstruction task, replacing placeholder implementation with real model execution. Update API to accept model_type parameter.

Purpose: Completes Phase 3 by wiring model wrappers to the Celery task, enabling end-to-end job submission with model selection. Includes verification checkpoint.

Output: Working reconstruction task that calls ReconViaGen/nvdiffrec models and produces mesh outputs.
</objective>

<execution_context>
@/home/devuser/.claude/get-shit-done/workflows/execute-plan.md
@/home/devuser/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-model-integration/03-CONTEXT.md
@.planning/phases/03-model-integration/03-RESEARCH.md
@.planning/phases/03-model-integration/03-01-SUMMARY.md
@.planning/phases/03-model-integration/03-02-SUMMARY.md
@.planning/phases/03-model-integration/03-03-SUMMARY.md

# Files to update
@app/tasks/reconstruction.py
@app/api/schemas.py
@app/api/jobs.py
@app/services/file_handler.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update reconstruction task with real model execution</name>
  <files>
    app/tasks/reconstruction.py
  </files>
  <action>
Replace placeholder reconstruction task with real model integration.

**Replace app/tasks/reconstruction.py with:**
```python
"""
Celery task for 3D reconstruction processing.

Executes ReconViaGen and/or nvdiffrec models to produce textured mesh outputs.
Supports single model execution or sequential execution of both models.
"""
import logging
import shutil
from pathlib import Path
from typing import Literal

from celery import shared_task
from celery.exceptions import SoftTimeLimitExceeded

from app.models import get_model, AVAILABLE_MODELS
from app.services.job_manager import is_job_cancelled, clear_cancellation
from app.services.file_handler import delete_job_files, get_job_path, STORAGE_ROOT
from app.services.vram_manager import cleanup_gpu_memory

logger = logging.getLogger(__name__)

# Model type including 'both' option
ModelSelection = Literal['reconviagen', 'nvdiffrec', 'both']


@shared_task(
    bind=True,
    name="reconstruction.process",
    soft_time_limit=7200,   # 2 hours soft limit (for 'both' mode)
    time_limit=7500,        # 2 hours 5 min hard limit
    acks_late=True,
    reject_on_worker_lost=True
)
def process_reconstruction(
    self,
    job_id: str,
    model_type: ModelSelection = "reconviagen"
) -> dict:
    """
    Process 3D reconstruction from uploaded images.

    Executes selected model(s) to produce textured mesh outputs.
    For 'both' mode, runs ReconViaGen first, then nvdiffrec sequentially
    with VRAM cleanup between models.

    Args:
        self: Celery task instance (bind=True provides this)
        job_id: Job identifier
        model_type: 'reconviagen', 'nvdiffrec', or 'both'

    Returns:
        dict: Result with status and output paths
            - {"status": "completed", "job_id": str, "outputs": dict}
            - {"status": "cancelled", "job_id": str}
            - {"status": "failed", "job_id": str, "error": str}
    """
    logger.info(f"Starting reconstruction job {job_id} with model_type={model_type}")

    # Get input directory
    job_dir = get_job_path(job_id)
    input_dir = job_dir

    if not input_dir.exists():
        logger.error(f"Job directory not found: {input_dir}")
        return {
            "status": "failed",
            "job_id": job_id,
            "error": "Job input files not found"
        }

    # Determine which models to run
    if model_type == 'both':
        models_to_run = ['reconviagen', 'nvdiffrec']
    else:
        models_to_run = [model_type]

    # Track results per model
    outputs = {}
    total_models = len(models_to_run)

    try:
        for model_index, current_model in enumerate(models_to_run):
            # Check for cancellation before each model
            if is_job_cancelled(job_id):
                logger.info(f"Job {job_id} cancelled before running {current_model}")
                delete_job_files(job_id)
                self.update_state(
                    state="REVOKED",
                    meta={"status": "cancelled", "job_id": job_id}
                )
                return {"status": "cancelled", "job_id": job_id}

            # Calculate progress offset for multi-model execution
            # Each model gets a portion of 0-100%
            progress_offset = int(100 * model_index / total_models)
            progress_scale = 100 / total_models

            logger.info(f"Running model {model_index + 1}/{total_models}: {current_model}")

            # Update progress: starting model
            self.update_state(
                state="PROGRESS",
                meta={
                    "progress": progress_offset,
                    "step": f"Starting {current_model}",
                    "model": current_model,
                    "model_index": model_index + 1,
                    "total_models": total_models
                }
            )

            # Create output directory for this model
            output_dir = job_dir / "output" / current_model
            output_dir.mkdir(parents=True, exist_ok=True)

            # Get and run model
            model = get_model(current_model, celery_task=self)

            try:
                # Load weights
                model.load_weights()

                # Check cancellation after weight loading
                if is_job_cancelled(job_id):
                    logger.info(f"Job {job_id} cancelled after loading {current_model}")
                    model.cleanup()
                    cleanup_gpu_memory()
                    delete_job_files(job_id)
                    return {"status": "cancelled", "job_id": job_id}

                # Run inference
                result = model.inference(input_dir, output_dir)

                # Cleanup model resources
                model.cleanup()

            except Exception as model_error:
                logger.error(f"Model {current_model} failed: {model_error}", exc_info=True)
                model.cleanup()
                cleanup_gpu_memory()

                # Per locked decision: stop on first failure
                return {
                    "status": "failed",
                    "job_id": job_id,
                    "error": f"Model failed to process images",
                    "model": current_model
                }

            # Check model result
            if result['status'] != 'success':
                logger.error(f"Model {current_model} returned failure: {result.get('error')}")

                # Per locked decision: stop on first failure
                return {
                    "status": "failed",
                    "job_id": job_id,
                    "error": result.get('error', 'Unknown model error'),
                    "model": current_model
                }

            # Store output paths
            outputs[current_model] = {
                "mesh_obj": result.get('mesh_path'),
                "mesh_ply": result.get('ply_path'),
                "texture": result.get('texture_path')
            }

            # CRITICAL: Cleanup VRAM before next model (for 'both' mode)
            if model_index < total_models - 1:
                logger.info(f"Cleaning up VRAM before next model")
                cleanup_gpu_memory()

                # Check cancellation between models
                if is_job_cancelled(job_id):
                    logger.info(f"Job {job_id} cancelled between models")
                    delete_job_files(job_id)
                    return {"status": "cancelled", "job_id": job_id}

        # All models completed successfully
        logger.info(f"Job {job_id} completed successfully with {total_models} model(s)")

        # Final progress update
        self.update_state(
            state="PROGRESS",
            meta={
                "progress": 100,
                "step": "Complete",
                "models_completed": list(outputs.keys())
            }
        )

        # Clear cancellation flags
        clear_cancellation(job_id)

        return {
            "status": "completed",
            "job_id": job_id,
            "outputs": outputs,
            "models_run": list(outputs.keys())
        }

    except SoftTimeLimitExceeded:
        logger.warning(f"Job {job_id} exceeded time limit")
        cleanup_gpu_memory()
        delete_job_files(job_id)
        return {
            "status": "failed",
            "job_id": job_id,
            "error": "Job exceeded time limit"
        }

    except Exception as e:
        logger.error(f"Unexpected error in job {job_id}: {e}", exc_info=True)
        cleanup_gpu_memory()
        return {
            "status": "failed",
            "job_id": job_id,
            "error": "An unexpected error occurred"
        }
```

**Key changes from placeholder:**
1. Imports and uses real model wrappers via get_model()
2. Handles 'both' mode with sequential execution
3. VRAM cleanup between models
4. Progress tracking includes model info
5. Proper timeout configuration (soft_time_limit, time_limit)
6. Error handling with user-friendly messages
7. Output directory structure: job_dir/output/reconviagen/ and job_dir/output/nvdiffrec/
  </action>
  <verify>
python -c "from app.tasks.reconstruction import process_reconstruction; print(process_reconstruction.name)"
grep -E "get_model|cleanup_gpu_memory" /home/devuser/3d-obj-rendering/app/tasks/reconstruction.py
  </verify>
  <done>
Reconstruction task uses real model wrappers via get_model().
Sequential model execution cleans VRAM between models.
Proper timeout and error handling configured.
Output directory structure matches spec (output/reconviagen/, output/nvdiffrec/).
  </done>
</task>

<task type="auto">
  <name>Task 2: Update API schemas with model_type</name>
  <files>
    app/api/schemas.py
    app/api/jobs.py
  </files>
  <action>
Add ModelType enum to schemas and update job submission to accept model_type parameter.

**Update app/api/schemas.py** (add ModelType enum and update JobStatusResponse):
```python
"""
Pydantic schemas for API requests and responses.
"""
from datetime import datetime
from enum import Enum
from typing import Literal, Optional

from pydantic import BaseModel, Field


class JobStatus(str, Enum):
    """Job status enumeration."""
    QUEUED = "queued"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class ModelType(str, Enum):
    """Available reconstruction model types."""
    RECONVIAGEN = "reconviagen"
    NVDIFFREC = "nvdiffrec"
    BOTH = "both"


class JobSubmitResponse(BaseModel):
    """Response when a job is submitted."""
    job_id: str = Field(..., description="8-character job identifier")
    status: JobStatus = Field(default=JobStatus.QUEUED, description="Job status (always 'queued' on submit)")
    model_type: ModelType = Field(..., description="Model type for reconstruction")
    created_at: datetime = Field(..., description="Job creation timestamp (ISO 8601)")

    model_config = {
        "json_schema_extra": {
            "example": {
                "job_id": "abc123xy",
                "status": "queued",
                "model_type": "reconviagen",
                "created_at": "2026-01-31T14:30:00Z"
            }
        }
    }


class JobStatusResponse(BaseModel):
    """Response for job status queries."""
    job_id: str = Field(..., description="Job identifier")
    status: JobStatus = Field(..., description="Current job status")
    progress: Optional[int] = Field(None, ge=0, le=100, description="Processing progress (0-100, only when processing)")
    current_model: Optional[str] = Field(None, description="Currently running model (when processing)")
    created_at: datetime = Field(..., description="Job creation timestamp")
    updated_at: Optional[datetime] = Field(None, description="Last update timestamp")
    error: Optional[str] = Field(None, description="Error message (only when failed)")

    model_config = {
        "json_schema_extra": {
            "example": {
                "job_id": "abc123xy",
                "status": "processing",
                "progress": 45,
                "current_model": "reconviagen",
                "created_at": "2026-01-31T14:30:00Z",
                "updated_at": "2026-01-31T14:32:15Z",
                "error": None
            }
        }
    }


class CancelRequest(BaseModel):
    """Request to cancel a job."""
    confirm: bool = Field(default=False, description="Must be true to confirm cancellation")

    model_config = {
        "json_schema_extra": {
            "example": {
                "confirm": True
            }
        }
    }


class CancelResponse(BaseModel):
    """Response for cancel requests."""
    job_id: str = Field(..., description="Job identifier")
    status: Literal["cancel_requested", "cancelled"] = Field(..., description="Cancellation status")
    message: str = Field(..., description="Human-readable message")

    model_config = {
        "json_schema_extra": {
            "example": {
                "job_id": "abc123xy",
                "status": "cancelled",
                "message": "Job cancelled successfully"
            }
        }
    }


class ErrorResponse(BaseModel):
    """Error response format."""
    error: str = Field(..., description="Error type or code")
    detail: Optional[str] = Field(None, description="Detailed error message")
    fields: Optional[dict[str, str]] = Field(None, description="Field-level validation errors")

    model_config = {
        "json_schema_extra": {
            "example": {
                "error": "ValidationError",
                "detail": "Invalid file upload",
                "fields": {
                    "views": "Expected 6 files, got 5"
                }
            }
        }
    }
```

**Update app/api/jobs.py** - Update submit_job endpoint to accept model_type:

Find the submit_job function and update to:
1. Add `model_type: ModelType = Form(ModelType.RECONVIAGEN)` parameter
2. Pass model_type to apply_async
3. Include model_type in response

The key changes in jobs.py:
```python
from app.api.schemas import (
    JobSubmitResponse, JobStatusResponse,
    CancelRequest, CancelResponse, ErrorResponse,
    JobStatus, ModelType  # Add ModelType
)

# In submit_job function:
async def submit_job(
    views: List[UploadFile] = File(..., description="6 multi-view images"),
    depth_renders: List[UploadFile] = File(..., description="6 depth render images"),
    model_type: ModelType = Form(ModelType.RECONVIAGEN, description="Model type: reconviagen, nvdiffrec, or both"),
):
    # ... validation code ...

    # Dispatch task with model_type
    process_reconstruction.apply_async(
        args=[job_id, model_type.value],
        task_id=job_id
    )

    return JobSubmitResponse(
        job_id=job_id,
        status=JobStatus.QUEUED,
        model_type=model_type,
        created_at=datetime.utcnow()
    )

# In get_job_status, extract current_model from task meta:
if task.state == "PROGRESS":
    meta = task.info or {}
    current_model = meta.get('model')
    # Include in response
```
  </action>
  <verify>
python -c "from app.api.schemas import ModelType; print(list(ModelType))"
grep -E "model_type|ModelType" /home/devuser/3d-obj-rendering/app/api/jobs.py
  </verify>
  <done>
ModelType enum added with reconviagen, nvdiffrec, both options.
JobSubmitResponse includes model_type field.
JobStatusResponse includes current_model field for progress tracking.
Jobs endpoint accepts model_type Form parameter.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify end-to-end model integration</name>
  <what-built>
Complete model integration with:
- Reconstruction task calling real model wrappers (currently STUBs)
- Model selection via API (reconviagen, nvdiffrec, both)
- Sequential execution with VRAM cleanup for 'both' mode
- Output directory structure with model subfolders
  </what-built>
  <how-to-verify>
**Prerequisites:** Docker compose must be rebuilt with new dependencies:
```bash
cd /home/devuser/3d-obj-rendering
docker-compose build
docker-compose up -d
```

**Test 1: Submit job with ReconViaGen model**
```bash
# Create test images (6 views + 6 depth)
mkdir -p /tmp/test_images
for i in {0..5}; do
  convert -size 256x256 xc:gray /tmp/test_images/view_0$i.png
  convert -size 256x256 xc:black /tmp/test_images/depth_0$i.png
done

# Submit job
curl -X POST http://localhost:8000/api/jobs \
  -F "model_type=reconviagen" \
  -F "views=@/tmp/test_images/view_00.png" \
  -F "views=@/tmp/test_images/view_01.png" \
  -F "views=@/tmp/test_images/view_02.png" \
  -F "views=@/tmp/test_images/view_03.png" \
  -F "views=@/tmp/test_images/view_04.png" \
  -F "views=@/tmp/test_images/view_05.png" \
  -F "depth_renders=@/tmp/test_images/depth_00.png" \
  -F "depth_renders=@/tmp/test_images/depth_01.png" \
  -F "depth_renders=@/tmp/test_images/depth_02.png" \
  -F "depth_renders=@/tmp/test_images/depth_03.png" \
  -F "depth_renders=@/tmp/test_images/depth_04.png" \
  -F "depth_renders=@/tmp/test_images/depth_05.png"

# Note job_id from response
```

**Test 2: Check job status (shows model progress)**
```bash
curl http://localhost:8000/api/jobs/{job_id}
# Should show status=processing with progress and current_model
# Wait for status=completed
```

**Test 3: Verify output files exist**
```bash
docker-compose exec api ls -la /app/storage/jobs/{job_id}/output/reconviagen/
# Should contain: mesh.obj, mesh.ply, mesh.mtl, mesh.png (texture)
```

**Test 4: Test 'both' mode**
```bash
# Submit with model_type=both
curl -X POST http://localhost:8000/api/jobs \
  -F "model_type=both" \
  # ... same files as above

# After completion, verify both output directories:
docker-compose exec api ls /app/storage/jobs/{job_id}/output/
# Should show: reconviagen/ and nvdiffrec/
```

**Expected results:**
1. Jobs submit successfully with model_type in response
2. Status endpoint shows current_model during processing
3. Output directories contain mesh files (OBJ, PLY, texture)
4. 'both' mode produces output in both model subdirectories
5. No CUDA OOM errors (STUBs don't use real GPU memory)

**Note:** Current implementation uses STUB models that produce placeholder meshes. Real model output will differ when official code is integrated.
  </how-to-verify>
  <resume-signal>Type "approved" if tests pass, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
Phase 3 success criteria verification:
1. User can select ReconViaGen model - YES (model_type=reconviagen)
2. User can select nvdiffrec model - YES (model_type=nvdiffrec)
3. Jobs complete with OBJ/PLY and texture files - YES (STUB output)
4. Model weights pre-downloaded in Docker - YES (directory structure created)
5. Both models run sequentially without VRAM overflow - YES (VRAM cleanup between models)

Note: Full model functionality pending official code release. STUB implementation validates all integration points.
</verification>

<success_criteria>
- Reconstruction task imports and uses model factory
- 'both' mode runs models sequentially with VRAM cleanup
- API accepts model_type parameter (reconviagen, nvdiffrec, both)
- Status endpoint includes current_model during processing
- Output files created in correct directory structure
- Checkpoint verification passes with test images
</success_criteria>

<output>
After completion, create `.planning/phases/03-model-integration/03-04-SUMMARY.md`
</output>
