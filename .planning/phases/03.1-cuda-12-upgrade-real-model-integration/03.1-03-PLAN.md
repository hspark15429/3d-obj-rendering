---
phase: 03.1-cuda-12-upgrade
plan: 03
type: execute
wave: 2
depends_on: ["03.1-01"]
files_modified:
  - Dockerfile
  - app/services/camera_estimation.py
autonomous: true

must_haves:
  truths:
    - "tiny-cuda-nn compiles and imports in container"
    - "Camera poses can be generated for nvdiffrec input format"
    - "NeRF-format transforms_train.json can be created from multi-view input"
  artifacts:
    - path: "Dockerfile"
      provides: "tiny-cuda-nn installation"
      contains: "tiny-cuda-nn"
    - path: "app/services/camera_estimation.py"
      provides: "Camera pose generation for nvdiffrec"
      min_lines: 80
  key_links:
    - from: "app/services/camera_estimation.py"
      to: "transforms_train.json"
      via: "create_nerf_dataset function"
      pattern: "transforms_train.json"
---

<objective>
Install nvdiffrec dependencies and create camera pose estimation service

Purpose: Enable nvdiffrec optimization loop which requires NeRF-format input with camera poses
Output: Dockerfile with tiny-cuda-nn, camera estimation service for pose generation
</objective>

<execution_context>
@/home/devuser/.claude/get-shit-done/workflows/execute-plan.md
@/home/devuser/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03.1-cuda-12-upgrade-real-model-integration/03.1-RESEARCH.md
@.planning/phases/03.1-cuda-12-upgrade-real-model-integration/03.1-01-SUMMARY.md
@Dockerfile
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add tiny-cuda-nn to Dockerfile</name>
  <files>Dockerfile</files>
  <action>
Add tiny-cuda-nn installation to Dockerfile after nvdiffrast:

1. Install tiny-cuda-nn from git (required by nvdiffrec for neural primitives):
```dockerfile
# Phase 3.1: nvdiffrec dependencies
# tiny-cuda-nn - neural network primitives for fast optimization
RUN pip install --no-cache-dir --no-build-isolation \
    git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch
```

Note: --no-build-isolation is REQUIRED because tiny-cuda-nn needs to find PyTorch during CUDA extension compilation.

2. Clone nvdiffrec repo for reference implementation:
```dockerfile
# Clone nvdiffrec for reference (geometry and rendering modules)
RUN git clone --depth 1 https://github.com/NVlabs/nvdiffrec.git /app/nvdiffrec_src
```

3. Add imageio for nvdiffrec image loading:
```dockerfile
RUN pip install --no-cache-dir imageio imageio-ffmpeg
```

Place these after the TRELLIS dependencies section in Dockerfile.
  </action>
  <verify>
Run: `docker build -t 3d-recon-test:nvdiffrec .`
Check: Build completes, tiny-cuda-nn compiles successfully
  </verify>
  <done>Dockerfile includes tiny-cuda-nn and nvdiffrec reference code</done>
</task>

<task type="auto">
  <name>Task 2: Create camera estimation service</name>
  <files>app/services/camera_estimation.py</files>
  <action>
Create camera estimation service that generates NeRF-format dataset for nvdiffrec:

```python
"""
Camera pose estimation and NeRF dataset preparation for nvdiffrec.

nvdiffrec expects input in NeRF synthetic dataset format:
- transforms_train.json with camera intrinsics and extrinsics
- Images with alpha channel (RGBA) for masking

Since our input has known camera poses (orthogonal views), we generate
canonical poses rather than estimating them with COLMAP/MASt3R.
"""
import json
import math
import logging
from pathlib import Path
from typing import List, Dict, Tuple
import shutil

import numpy as np
from PIL import Image

logger = logging.getLogger(__name__)

# Standard orthogonal view directions (camera looks at origin)
# Order: front (+Z), back (-Z), right (+X), left (-X), top (+Y), bottom (-Y)
CANONICAL_VIEWS = [
    {"name": "front",  "position": [0, 0, 2.5],  "up": [0, 1, 0]},
    {"name": "back",   "position": [0, 0, -2.5], "up": [0, 1, 0]},
    {"name": "right",  "position": [2.5, 0, 0],  "up": [0, 1, 0]},
    {"name": "left",   "position": [-2.5, 0, 0], "up": [0, 1, 0]},
    {"name": "top",    "position": [0, 2.5, 0],  "up": [0, 0, -1]},
    {"name": "bottom", "position": [0, -2.5, 0], "up": [0, 0, 1]},
]


def look_at_matrix(eye: List[float], target: List[float], up: List[float]) -> np.ndarray:
    """
    Create a look-at camera matrix (camera-to-world transform).

    Args:
        eye: Camera position [x, y, z]
        target: Look-at target [x, y, z]
        up: Up vector [x, y, z]

    Returns:
        4x4 camera-to-world transformation matrix (OpenGL convention)
    """
    eye = np.array(eye, dtype=np.float64)
    target = np.array(target, dtype=np.float64)
    up = np.array(up, dtype=np.float64)

    # Forward vector (camera looks along -Z in OpenGL)
    forward = target - eye
    forward = forward / np.linalg.norm(forward)

    # Right vector
    right = np.cross(forward, up)
    right = right / np.linalg.norm(right)

    # Recompute up to ensure orthogonality
    up = np.cross(right, forward)

    # Build rotation matrix (columns are right, up, -forward)
    rotation = np.eye(4, dtype=np.float64)
    rotation[:3, 0] = right
    rotation[:3, 1] = up
    rotation[:3, 2] = -forward  # OpenGL convention

    # Build translation
    translation = np.eye(4, dtype=np.float64)
    translation[:3, 3] = eye

    # Camera-to-world = translation @ rotation
    return translation @ rotation


def compute_fov_x(image_width: int, focal_length: float) -> float:
    """Compute horizontal field of view in radians."""
    return 2 * math.atan(image_width / (2 * focal_length))


def create_nerf_dataset(
    views_dir: Path,
    depth_dir: Path,
    output_dir: Path,
    image_size: int = 512,
    focal_length: float = 1111.0,  # Default for 512px with ~50 degree FOV
) -> Dict:
    """
    Convert multi-view images to NeRF synthetic dataset format.

    Args:
        views_dir: Directory with view_00.png ... view_05.png
        depth_dir: Directory with depth_00.png ... depth_05.png (for masking)
        output_dir: Where to write transforms_train.json and images
        image_size: Output image resolution (images will be resized)
        focal_length: Camera focal length in pixels

    Returns:
        Dict with status and paths
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Create images subdirectory
    images_out = output_dir / "images"
    images_out.mkdir(exist_ok=True)

    # Compute camera angle (FOV)
    camera_angle_x = compute_fov_x(image_size, focal_length)

    frames = []
    view_files = sorted(Path(views_dir).glob("view_*.png"))

    if len(view_files) != 6:
        return {
            "status": "failed",
            "error": f"Expected 6 view files, found {len(view_files)}"
        }

    for i, (view_file, view_config) in enumerate(zip(view_files, CANONICAL_VIEWS)):
        # Load and process image
        img = Image.open(view_file).convert("RGBA")

        # Resize if needed
        if img.size[0] != image_size or img.size[1] != image_size:
            img = img.resize((image_size, image_size), Image.LANCZOS)

        # Try to use depth for alpha mask
        depth_file = Path(depth_dir) / f"depth_{i:02d}.png"
        if depth_file.exists():
            depth = Image.open(depth_file).convert("L")
            depth = depth.resize((image_size, image_size), Image.LANCZOS)
            # Non-zero depth = object, zero depth = background
            depth_array = np.array(depth)
            alpha = (depth_array > 0).astype(np.uint8) * 255
            img_array = np.array(img)
            img_array[:, :, 3] = alpha
            img = Image.fromarray(img_array)

        # Save processed image
        out_name = f"view_{i:02d}.png"
        img.save(images_out / out_name)

        # Create camera transform matrix
        transform_matrix = look_at_matrix(
            eye=view_config["position"],
            target=[0, 0, 0],  # Look at origin
            up=view_config["up"]
        )

        frames.append({
            "file_path": f"./images/{out_name}",
            "transform_matrix": transform_matrix.tolist()
        })

        logger.debug(f"Processed view {i}: {view_config['name']}")

    # Write transforms_train.json
    transforms = {
        "camera_angle_x": camera_angle_x,
        "frames": frames
    }

    transforms_path = output_dir / "transforms_train.json"
    with open(transforms_path, "w") as f:
        json.dump(transforms, f, indent=2)

    logger.info(f"Created NeRF dataset with {len(frames)} views at {output_dir}")

    return {
        "status": "success",
        "transforms_path": str(transforms_path),
        "image_count": len(frames),
        "image_size": image_size
    }


def validate_nerf_dataset(dataset_dir: Path) -> Dict:
    """
    Validate a NeRF-format dataset for nvdiffrec compatibility.

    Args:
        dataset_dir: Directory containing transforms_train.json

    Returns:
        Dict with validation status and details
    """
    dataset_dir = Path(dataset_dir)
    transforms_path = dataset_dir / "transforms_train.json"

    if not transforms_path.exists():
        return {"valid": False, "error": "transforms_train.json not found"}

    try:
        with open(transforms_path) as f:
            transforms = json.load(f)

        # Check required fields
        if "camera_angle_x" not in transforms:
            return {"valid": False, "error": "Missing camera_angle_x"}

        if "frames" not in transforms:
            return {"valid": False, "error": "Missing frames array"}

        frames = transforms["frames"]
        if len(frames) < 1:
            return {"valid": False, "error": "No frames in dataset"}

        # Validate each frame
        for i, frame in enumerate(frames):
            if "file_path" not in frame:
                return {"valid": False, "error": f"Frame {i} missing file_path"}
            if "transform_matrix" not in frame:
                return {"valid": False, "error": f"Frame {i} missing transform_matrix"}

            # Check image exists
            img_path = dataset_dir / frame["file_path"]
            if not img_path.exists():
                return {"valid": False, "error": f"Image not found: {frame['file_path']}"}

            # Check transform matrix shape
            matrix = frame["transform_matrix"]
            if len(matrix) != 4 or any(len(row) != 4 for row in matrix):
                return {"valid": False, "error": f"Frame {i} transform_matrix not 4x4"}

        return {
            "valid": True,
            "frame_count": len(frames),
            "camera_angle_x": transforms["camera_angle_x"]
        }

    except json.JSONDecodeError as e:
        return {"valid": False, "error": f"Invalid JSON: {e}"}
    except Exception as e:
        return {"valid": False, "error": str(e)}
```

This service:
- Generates canonical camera poses for our 6-view input
- Creates NeRF-format transforms_train.json
- Resizes images and adds alpha channel from depth
- Validates generated dataset
  </action>
  <verify>
Run: `python -c "from app.services.camera_estimation import create_nerf_dataset, validate_nerf_dataset; print('Import OK')"`
Check: No import errors
  </verify>
  <done>Camera estimation service created with NeRF dataset generation</done>
</task>

<task type="auto">
  <name>Task 3: Verify nvdiffrec dependencies work</name>
  <files></files>
  <action>
Build image and verify nvdiffrec dependencies:

1. Build updated image:
```bash
docker build -t 3d-recon-test:nvdiffrec .
```

2. Test tiny-cuda-nn import:
```bash
docker run --gpus all --rm 3d-recon-test:nvdiffrec python -c "
import tinycudann as tcnn
print(f'tiny-cuda-nn imported successfully')
# Quick network test
import torch
network = tcnn.Network(
    n_input_dims=3,
    n_output_dims=4,
    network_config={
        'otype': 'FullyFusedMLP',
        'activation': 'ReLU',
        'output_activation': 'None',
        'n_neurons': 64,
        'n_hidden_layers': 2,
    }
)
x = torch.randn(100, 3).cuda()
y = network(x)
print(f'Network output shape: {y.shape}')
"
```

3. Test nvdiffrast (should still work from Plan 01):
```bash
docker run --gpus all --rm 3d-recon-test:nvdiffrec python -c "
import nvdiffrast.torch as dr
ctx = dr.RasterizeCudaContext()
print('nvdiffrast context created')
"
```

4. Test camera estimation service:
```bash
docker run --gpus all --rm 3d-recon-test:nvdiffrec python -c "
from app.services.camera_estimation import look_at_matrix, compute_fov_x
import numpy as np

# Test look_at_matrix
mat = look_at_matrix([0, 0, 2.5], [0, 0, 0], [0, 1, 0])
print(f'Look-at matrix shape: {mat.shape}')
assert mat.shape == (4, 4)

# Test FOV computation
fov = compute_fov_x(512, 1111.0)
print(f'FOV: {np.degrees(fov):.1f} degrees')
assert 40 < np.degrees(fov) < 60  # Reasonable range

print('Camera estimation tests passed')
"
```

5. Test existing tests still pass:
```bash
docker run --gpus all --rm 3d-recon-test:nvdiffrec pytest tests/ -v
```
  </action>
  <verify>
All verification commands succeed:
- tiny-cuda-nn network operations work
- nvdiffrast context creates successfully
- Camera estimation functions produce valid matrices
- Existing tests pass
  </verify>
  <done>nvdiffrec dependencies verified working (tiny-cuda-nn, nvdiffrast, camera estimation)</done>
</task>

</tasks>

<verification>
- Docker build succeeds with tiny-cuda-nn
- tiny-cuda-nn network operations work on GPU
- nvdiffrast context creation succeeds
- Camera estimation service generates valid look-at matrices
- Existing tests pass
</verification>

<success_criteria>
- Dockerfile includes tiny-cuda-nn installation
- nvdiffrec reference code cloned to container
- app/services/camera_estimation.py provides create_nerf_dataset function
- All nvdiffrec dependencies work together in container
</success_criteria>

<output>
After completion, create `.planning/phases/03.1-cuda-12-upgrade-real-model-integration/03.1-03-SUMMARY.md`
</output>
