---
phase: 04-quality-preview
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/mesh_renderer.py
autonomous: true

must_haves:
  truths:
    - "Mesh can be rendered from a given camera pose"
    - "Depth map can be rendered from a given camera pose"
    - "Rendered images are vertically flipped to match standard top-down ordering"
    - "Camera poses can be loaded from transforms_train.json"
  artifacts:
    - path: "app/services/mesh_renderer.py"
      provides: "nvdiffrast mesh and depth rendering"
      exports: ["MeshRenderer", "render_mesh", "render_depth", "load_camera_poses"]
      min_lines: 150
  key_links:
    - from: "app/services/mesh_renderer.py"
      to: "nvdiffrast.torch"
      via: "import"
      pattern: "import nvdiffrast"
    - from: "app/services/mesh_renderer.py"
      to: "transforms_train.json"
      via: "json.load"
      pattern: "transforms_train.json"
---

<objective>
Create mesh rendering service using nvdiffrast for comparison rendering and preview generation.

Purpose: Enable rendering meshes from specific camera poses for quality metric computation (comparing rendered output to input images) and preview image generation.

Output: mesh_renderer.py service with MeshRenderer class and render functions.
</objective>

<execution_context>
@/home/devuser/.claude/get-shit-done/workflows/execute-plan.md
@/home/devuser/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-quality-preview/04-CONTEXT.md
@.planning/phases/04-quality-preview/04-RESEARCH.md
@app/services/camera_estimation.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create mesh_renderer.py service</name>
  <files>app/services/mesh_renderer.py</files>
  <action>
Create app/services/mesh_renderer.py with nvdiffrast rendering capabilities:

1. **load_camera_poses(dataset_dir: Path) -> dict**
   - Load transforms_train.json from dataset_dir
   - Return {"camera_angle_x": float, "frames": [{"transform_matrix": np.ndarray, "file_path": str}]}
   - Reuse pattern from camera_estimation.py

2. **build_mvp_matrix(transform_matrix: np.ndarray, camera_angle_x: float, resolution: Tuple[int, int]) -> torch.Tensor**
   - Build Model-View-Projection matrix from NeRF transform
   - View matrix = inverse of camera-to-world transform
   - Projection matrix = perspective from camera_angle_x
   - Return 4x4 torch tensor

3. **class MeshRenderer:**
   - __init__(self, device: str = "cuda"):
     - Initialize nvdiffrast GL context (dr.RasterizeGLContext or dr.RasterizeCudaContext)
     - Store device

   - load_mesh(self, glb_path: Path) -> dict:
     - Load GLB file using trimesh
     - Extract vertices, faces, UVs, texture
     - Convert to torch tensors on device
     - Return {"vertices": tensor, "faces": tensor, "uvs": tensor, "texture": tensor}

   - render_textured(self, mesh: dict, mvp: torch.Tensor, resolution: Tuple[int, int]) -> np.ndarray:
     - Transform vertices to clip space: pos_clip = vertices @ mvp.T
     - Add homogeneous coordinate if needed
     - Rasterize with dr.rasterize()
     - Interpolate UVs with dr.interpolate()
     - Sample texture with dr.texture()
     - CRITICAL: Flip output vertically (OpenGL uses bottom-up)
     - Return (H, W, 3) numpy array in [0, 1]

   - render_depth(self, mesh: dict, mvp: torch.Tensor, resolution: Tuple[int, int]) -> np.ndarray:
     - Transform vertices to clip space
     - Rasterize to get depth buffer
     - Extract depth from rasterization output
     - CRITICAL: Flip output vertically
     - Return (H, W) numpy array with depth values

   - render_wireframe(self, mesh: dict, mvp: torch.Tensor, resolution: Tuple[int, int], line_color: Tuple[int, int, int] = (0, 0, 0), bg_color: Tuple[int, int, int] = (255, 255, 255)) -> np.ndarray:
     - Extract mesh edges using trimesh.edges_unique pattern
     - Render edges as lines (use anti-aliased rasterization or point splatting)
     - For simplicity: render filled mesh with edge overlay
     - CRITICAL: Flip output vertically
     - Return (H, W, 3) numpy array

4. **Standalone functions for convenience:**
   - render_mesh(glb_path, transform_matrix, camera_angle_x, resolution) -> np.ndarray
   - render_depth(glb_path, transform_matrix, camera_angle_x, resolution) -> np.ndarray

IMPORTANT PITFALLS TO AVOID (from RESEARCH.md):
- Always flip rendered images vertically: `rendered = rendered[::-1, :, :]`
- Handle meshes without UVs gracefully (use vertex colors or solid color)
- Use float32 for vertex positions, int32 for face indices
- Ensure homogeneous coordinates (w=1) for clip space

Include logging for render operations.
  </action>
  <verify>
python -c "from app.services.mesh_renderer import MeshRenderer, load_camera_poses; print('imports ok')"
  </verify>
  <done>
mesh_renderer.py exists with MeshRenderer class and render functions, nvdiffrast integration works
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit test for mesh renderer</name>
  <files>tests/test_mesh_renderer.py</files>
  <action>
Create tests/test_mesh_renderer.py with basic validation tests:

1. test_load_camera_poses():
   - Create minimal transforms_train.json in temp directory
   - Verify load_camera_poses returns correct structure
   - Check camera_angle_x and frames list

2. test_build_mvp_matrix():
   - Test with identity transform
   - Verify output is 4x4 tensor
   - Check that projection is applied correctly

3. test_mesh_renderer_initialization():
   - Skip if CUDA not available (pytest.mark.skipif)
   - Verify MeshRenderer can be instantiated
   - Check context is created

Note: Full rendering tests require GPU and mesh files, so keep tests minimal.
Use pytest.mark.skipif for GPU-dependent tests.
  </action>
  <verify>pytest tests/test_mesh_renderer.py -v --ignore=tests/test_integration.py</verify>
  <done>test_mesh_renderer.py exists with passing tests for non-GPU functions</done>
</task>

</tasks>

<verification>
1. app/services/mesh_renderer.py exists with MeshRenderer class
2. Python imports work without error
3. Non-GPU unit tests pass
4. Vertical flip applied to all rendered outputs
</verification>

<success_criteria>
- mesh_renderer.py provides MeshRenderer class with render_textured, render_depth, render_wireframe methods
- Camera pose loading reuses transforms_train.json format from camera_estimation.py
- All rendered images are flipped vertically (OpenGL convention handled)
- Basic unit tests pass for non-GPU functions
</success_criteria>

<output>
After completion, create `.planning/phases/04-quality-preview/04-02-SUMMARY.md`
</output>
