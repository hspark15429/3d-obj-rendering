---
phase: 04-quality-preview
plan: 04
type: execute
wave: 3
depends_on: ["04-03"]
files_modified:
  - app/tasks/reconstruction.py
  - tests/test_quality_pipeline.py
autonomous: true

must_haves:
  truths:
    - "Reconstruction task calls quality pipeline after model completion"
    - "Quality.json is saved in job output directory"
    - "Preview images are saved in job output directory"
    - "Job result includes quality status in output"
    - "If quality pipeline fails, job fails"
  artifacts:
    - path: "app/tasks/reconstruction.py"
      provides: "Reconstruction task with quality integration"
      contains: "PreviewGenerator"
    - path: "tests/test_quality_pipeline.py"
      provides: "Quality pipeline integration tests"
      min_lines: 50
  key_links:
    - from: "app/tasks/reconstruction.py"
      to: "app/services/preview_generator.py"
      via: "import"
      pattern: "from app.services.preview_generator import"
---

<objective>
Integrate quality pipeline into reconstruction task and add verification tests.

Purpose: Wire up the quality metrics and preview generation into the main job pipeline so completed reconstructions include quality reports and preview images.

Output: Modified reconstruction.py with quality pipeline integration, plus integration tests.
</objective>

<execution_context>
@/home/devuser/.claude/get-shit-done/workflows/execute-plan.md
@/home/devuser/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-quality-preview/04-CONTEXT.md
@.planning/phases/04-quality-preview/04-RESEARCH.md
@.planning/phases/04-quality-preview/04-03-SUMMARY.md
@app/tasks/reconstruction.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate quality pipeline into reconstruction task</name>
  <files>app/tasks/reconstruction.py</files>
  <action>
Modify app/tasks/reconstruction.py to call quality pipeline after model completion:

1. Add import at top:
   ```python
   from app.services.preview_generator import PreviewGenerator
   ```

2. After model inference completes successfully (around line 163 where outputs dict is built), add quality pipeline:
   ```python
   # Run quality pipeline for completed model
   try:
       self.update_state(
           state="PROGRESS",
           meta={
               "progress": progress_offset + int(progress_scale * 0.8),
               "step": f"Computing quality metrics for {current_model}",
               "model": current_model
           }
       )

       preview_gen = PreviewGenerator()
       quality_result = preview_gen.generate_all(
           job_id=job_id,
           model=current_model,
           mesh_path=Path(result.get('mesh_path')),  # GLB path
           input_dir=input_dir,
           output_dir=output_dir,
           metadata={
               "input_file": "uploaded.zip",  # or extract from job metadata
               "duration": None  # TODO: track actual duration
           }
       )

       # Add quality info to outputs
       outputs[current_model]["quality"] = quality_result["quality_report"]
       outputs[current_model]["previews"] = quality_result["previews"]
       outputs[current_model]["quality_status"] = quality_result["status"]

   except Exception as quality_error:
       logger.error(f"Quality pipeline failed for {current_model}: {quality_error}", exc_info=True)
       # Per CONTEXT.md: quality metrics are required, if rendering fails job fails
       return {
           "status": "failed",
           "job_id": job_id,
           "error": f"Quality metrics computation failed: {str(quality_error)}",
           "model": current_model
       }
   ```

3. Update final result structure to include quality status:
   ```python
   return {
       "status": "completed",
       "job_id": job_id,
       "outputs": outputs,
       "models_run": list(outputs.keys()),
       "quality_status": outputs[list(outputs.keys())[-1]].get("quality_status", "unknown")
   }
   ```

IMPORTANT per CONTEXT.md:
- Quality metrics are REQUIRED - if rendering fails, job fails (no partial success)
- Always save mesh regardless of quality status
- Track quality pipeline in progress updates
  </action>
  <verify>
python -c "from app.tasks.reconstruction import process_reconstruction; print('import ok')"
  </verify>
  <done>
reconstruction.py imports PreviewGenerator and calls quality pipeline after model completion
  </done>
</task>

<task type="auto">
  <name>Task 2: Create quality pipeline integration test</name>
  <files>tests/test_quality_pipeline.py</files>
  <action>
Create tests/test_quality_pipeline.py with integration tests:

1. test_quality_metrics_computation():
   - Create two test images (one solid color, one slightly different)
   - Compute PSNR/SSIM using quality_metrics functions
   - Verify PSNR is finite, SSIM is in [0, 1]

2. test_classify_quality_status():
   - Test normal: PSNR=30, SSIM=0.9 -> "normal"
   - Test warning: PSNR=22, SSIM=0.8 -> "warning"
   - Test failure: PSNR=15, SSIM=0.6 -> "failure"
   - Test edge cases at thresholds

3. test_quality_report_structure():
   - Call generate_quality_report with mock data
   - Verify JSON structure matches spec
   - Check required fields: job_id, model, status, summary, metrics, thresholds, metadata

4. test_depth_error_computation():
   - Create test depth maps (one with zeros for background)
   - Compute depth error metrics
   - Verify MAE/RMSE are non-negative
   - Verify valid_pixels count excludes background

Use pytest fixtures for test data.
Mark GPU-dependent tests with @pytest.mark.skipif.
  </action>
  <verify>pytest tests/test_quality_pipeline.py -v --ignore=tests/test_integration.py</verify>
  <done>test_quality_pipeline.py exists with passing tests for quality metrics and report generation</done>
</task>

<task type="auto">
  <name>Task 3: Verify all tests pass</name>
  <files>N/A</files>
  <action>
Run all existing tests to ensure no regressions:

1. Run full test suite (excluding integration tests that need Docker):
   ```bash
   pytest tests/ -v --ignore=tests/test_integration.py
   ```

2. Verify all tests pass, including new quality pipeline tests

3. If any tests fail, fix the issues before completing this plan
  </action>
  <verify>pytest tests/ -v --ignore=tests/test_integration.py exits with code 0</verify>
  <done>All tests pass, no regressions from quality pipeline integration</done>
</task>

</tasks>

<verification>
1. reconstruction.py imports and uses PreviewGenerator
2. Quality pipeline is called after model completion
3. Job output includes quality_status, quality report, and preview paths
4. Quality failure causes job failure (per CONTEXT.md)
5. All tests pass including new quality pipeline tests
</verification>

<success_criteria>
- reconstruction.py integrates PreviewGenerator.generate_all() after model inference
- Job result includes quality_status at top level
- Quality/preview paths included in outputs dict
- New integration tests verify quality metrics, classification, and report structure
- All existing tests continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-quality-preview/04-04-SUMMARY.md`
</output>
