---
phase: 02-job-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - requirements.txt
  - app/celery_app.py
  - app/config.py
autonomous: true

must_haves:
  truths:
    - "Celery worker connects to Redis broker"
    - "Redis service runs and accepts connections"
    - "Worker logs show successful connection on startup"
  artifacts:
    - path: "app/celery_app.py"
      provides: "Celery application instance"
      exports: ["celery_app"]
    - path: "app/config.py"
      provides: "Application settings"
      contains: "CELERY_BROKER_URL"
    - path: "docker-compose.yml"
      provides: "Redis and worker services"
      contains: "redis:"
  key_links:
    - from: "app/celery_app.py"
      to: "redis://redis:6379/0"
      via: "broker configuration"
      pattern: "broker.*redis"
    - from: "docker-compose.yml"
      to: "celery -A app.celery_app worker"
      via: "worker command"
      pattern: "celery.*worker"
---

<objective>
Set up Celery infrastructure with Redis broker for async job processing.

Purpose: Enables long-running reconstruction tasks to run asynchronously without blocking the API. This is the foundation for all job queue functionality.
Output: Redis and Celery worker services in docker-compose, celery_app.py with shared_task support, config.py with settings.
</objective>

<execution_context>
@/home/devuser/.claude/get-shit-done/workflows/execute-plan.md
@/home/devuser/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-job-pipeline/02-CONTEXT.md
@.planning/phases/02-job-pipeline/02-RESEARCH.md
@app/main.py
@docker-compose.yml
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Celery app and configuration</name>
  <files>app/celery_app.py, app/config.py</files>
  <action>
Create app/config.py with settings class:
- CELERY_BROKER_URL: redis://redis:6379/0 (from env with default)
- CELERY_RESULT_BACKEND: redis://redis:6379/0
- REDIS_STATE_DB: redis://redis:6379/1 (separate DB for app state like cancellation flags)
- JOB_STORAGE_PATH: /app/storage/jobs
- Use pydantic-settings BaseSettings for env var loading

Create app/celery_app.py:
- Use create_celery_app() factory pattern from research
- Configure: task_track_started=True, task_serializer="json", result_serializer="json"
- Set visibility_timeout to 14400 (4 hours) for long inference tasks
- Set worker_prefetch_multiplier=1 for fair task distribution
- Include autodiscover for app.tasks module
- Export celery_app instance

DO NOT use @celery.task decorator (causes circular imports). The shared_task decorator will be used in tasks/ modules.
  </action>
  <verify>
python -c "from app.celery_app import celery_app; print(celery_app.conf.broker_url)"
python -c "from app.config import settings; print(settings.CELERY_BROKER_URL)"
  </verify>
  <done>
celery_app imports without errors and shows redis broker URL. config.py provides settings via pydantic-settings.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add Redis and Worker services to Docker Compose</name>
  <files>docker-compose.yml, requirements.txt</files>
  <action>
Update requirements.txt - add:
- celery[redis]>=5.3.0,<6.0
- redis>=5.0.0
- pydantic-settings>=2.0.0
- python-multipart>=0.0.9
- aiofiles>=24.0.0
- nanoid>=2.0.0
- filetype>=1.2.0

Update docker-compose.yml - add services:

1. redis service:
   - image: redis:7-alpine
   - ports: 6379:6379
   - healthcheck: redis-cli ping, interval 10s, timeout 5s, retries 3
   - volumes: redis-data:/data for persistence

2. worker service:
   - build: . (same Dockerfile as api)
   - command: celery -A app.celery_app worker --loglevel=info --concurrency=1
   - environment: same CELERY_* env vars as api
   - depends_on: redis (healthy)
   - volumes: shared storage volume with api
   - GPU reservation same as api (worker needs GPU for inference)

3. Update api service:
   - depends_on: redis (healthy)
   - Add CELERY_BROKER_URL, CELERY_RESULT_BACKEND env vars
   - Add shared storage volume

4. Add volumes section:
   - redis-data: for Redis persistence
   - job-storage: shared between api and worker at /app/storage

DO NOT add flower (monitoring) - keep it minimal for now.
  </action>
  <verify>
docker-compose config (validates compose file syntax)
grep -q "redis:" docker-compose.yml && echo "Redis service found"
grep -q "worker:" docker-compose.yml && echo "Worker service found"
  </verify>
  <done>
docker-compose.yml has redis and worker services. requirements.txt includes celery[redis] and supporting libraries. docker-compose config validates without errors.
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify Celery infrastructure starts correctly</name>
  <files>None (verification only)</files>
  <action>
Run docker-compose up --build -d to start all services.
Wait for services to be healthy.
Check worker logs to verify Celery connects to Redis.
Verify redis responds to ping.
Run a quick connectivity test.

Expected:
- Redis responds to PING
- Worker shows "celery@... ready" message
- API still responds to /health

If worker fails to connect, check:
1. Redis is healthy first
2. Broker URL matches between services
3. No import errors in celery_app.py
  </action>
  <verify>
docker-compose up --build -d
sleep 10
docker-compose logs worker 2>&1 | grep -E "(ready|connected|started)"
docker-compose exec redis redis-cli ping
curl -s http://localhost:8000/health | grep -q "healthy"
  </verify>
  <done>
All three services running: redis healthy, worker connected and ready, api healthy. Worker logs show successful Redis connection.
  </done>
</task>

</tasks>

<verification>
- docker-compose ps shows 3 services (api, redis, worker) all healthy/running
- Worker logs show "celery@... ready" and connected to Redis
- redis-cli ping returns PONG
- API /health returns 200 with GPU info
- No import errors in any service logs
</verification>

<success_criteria>
1. Redis service starts and accepts connections
2. Celery worker connects to Redis broker successfully
3. API service remains functional with /health endpoint working
4. celery_app.py can be imported and shows correct configuration
5. All services restart cleanly without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-job-pipeline/02-01-SUMMARY.md`
</output>
