---
phase: 02-job-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - app/services/job_manager.py
  - app/tasks/__init__.py
  - app/tasks/reconstruction.py
autonomous: true

must_haves:
  truths:
    - "Job manager tracks cancellation requests in Redis"
    - "Celery task checks for cancellation at each step"
    - "Task reports progress percentage via update_state"
    - "Cancelled tasks clean up their files"
  artifacts:
    - path: "app/services/job_manager.py"
      provides: "Redis-based cancellation tracking"
      exports: ["request_cancellation", "confirm_cancellation", "is_job_cancelled"]
    - path: "app/tasks/reconstruction.py"
      provides: "Celery task with progress tracking"
      exports: ["process_reconstruction"]
  key_links:
    - from: "app/tasks/reconstruction.py"
      to: "app/services/job_manager.py"
      via: "is_job_cancelled check"
      pattern: "is_job_cancelled"
    - from: "app/tasks/reconstruction.py"
      to: "self.update_state"
      via: "progress reporting"
      pattern: "update_state.*PROGRESS"
---

<objective>
Create job manager for cancellation tracking and Celery task with progress reporting.

Purpose: The job manager implements two-step cancellation via Redis flags. The Celery task demonstrates the progress tracking pattern that will wrap actual model inference in Phase 3.
Output: job_manager.py with cancellation logic, reconstruction.py task with placeholder steps and progress updates.
</objective>

<execution_context>
@/home/devuser/.claude/get-shit-done/workflows/execute-plan.md
@/home/devuser/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-job-pipeline/02-CONTEXT.md
@.planning/phases/02-job-pipeline/02-RESEARCH.md
@.planning/phases/02-job-pipeline/02-01-SUMMARY.md
@app/celery_app.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create job manager with Redis-based cancellation</name>
  <files>app/services/job_manager.py</files>
  <action>
Create app/services/job_manager.py with:

1. Import redis and create connection using config.REDIS_STATE_DB (DB 1, separate from Celery broker).

2. def get_redis_client() -> redis.Redis:
   - Parse REDIS_STATE_DB URL and return client
   - Use lazy initialization (create on first call, cache in module)

3. def request_cancellation(job_id: str) -> bool:
   - Set cancel_request:{job_id} = "pending" with 1 hour TTL
   - Return True (always succeeds)
   - This is step 1 of two-step cancel

4. def confirm_cancellation(job_id: str) -> bool:
   - Check if cancel_request:{job_id} exists
   - If yes: set cancel:{job_id} = "1" with 1 hour TTL, delete cancel_request key, return True
   - If no: return False (nothing to confirm)

5. def is_job_cancelled(job_id: str) -> bool:
   - Return True if cancel:{job_id} exists in Redis
   - Called by worker to check if should abort

6. def cancel_pending(job_id: str) -> bool:
   - Return True if cancel_request:{job_id} exists (cancel requested but not confirmed)

7. def clear_cancellation(job_id: str) -> None:
   - Delete both cancel_request:{job_id} and cancel:{job_id} keys
   - Called after job completes to clean up

Use 3600 second TTL (1 hour) for all cancellation keys - auto-cleanup for abandoned cancels.
  </action>
  <verify>
python -c "
from app.services.job_manager import (
    request_cancellation, confirm_cancellation, is_job_cancelled,
    cancel_pending, clear_cancellation
)
print('All job_manager functions imported successfully')
"
  </verify>
  <done>
job_manager.py imports without errors. All 5 functions are defined. Uses Redis DB 1 (separate from Celery).
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Celery reconstruction task with progress tracking</name>
  <files>app/tasks/__init__.py, app/tasks/reconstruction.py</files>
  <action>
Create app/tasks/__init__.py (empty, package marker).

Create app/tasks/reconstruction.py with:

1. Import shared_task from celery (NOT from app.celery_app)
2. Import is_job_cancelled, clear_cancellation from job_manager
3. Import delete_job_files from file_handler

4. @shared_task(bind=True, name="reconstruction.process")
   def process_reconstruction(self, job_id: str, model_type: str = "reconviagen") -> dict:

   Implement placeholder steps (Phase 3 will add real model calls):

   steps = [
       ("Loading input files", 10),
       ("Preprocessing images", 20),
       ("Running reconstruction", 60),  # This will be the real model call
       ("Post-processing mesh", 80),
       ("Generating previews", 90),
       ("Finalizing output", 100),
   ]

   For each step:
   - Check is_job_cancelled(job_id) FIRST
   - If cancelled:
     - Call delete_job_files(job_id)
     - Update state to REVOKED
     - Return {"status": "cancelled", "job_id": job_id}
   - Otherwise:
     - Log step name
     - Call self.update_state(state="PROGRESS", meta={"progress": percent, "step": step_name})
     - time.sleep(2) as placeholder (simulates work)

   After all steps:
   - clear_cancellation(job_id) to clean up Redis keys
   - Return {"status": "completed", "job_id": job_id, "output_path": f"/jobs/{job_id}/output"}

Use logging to show step progress. This is a PLACEHOLDER - Phase 3 replaces sleep with real model calls.
  </action>
  <verify>
python -c "
from app.tasks.reconstruction import process_reconstruction
print('Task name:', process_reconstruction.name)
print('Task bound:', process_reconstruction.bind)
"
  </verify>
  <done>
reconstruction.py imports without errors. Task is named "reconstruction.process" and has bind=True. Task includes cancellation checks and progress updates.
  </done>
</task>

<task type="auto">
  <name>Task 3: Test task execution in Docker environment</name>
  <files>None (verification only)</files>
  <action>
Rebuild and restart Docker services to pick up new code:
docker-compose down
docker-compose up --build -d

Wait for services to be healthy, then verify:

1. Check worker discovers the task:
   docker-compose logs worker | grep "reconstruction.process"

2. Test task can be queued (dry run without full execution):
   docker-compose exec api python -c "
   from app.tasks.reconstruction import process_reconstruction
   result = process_reconstruction.delay('test123', 'reconviagen')
   print('Task ID:', result.id)
   print('Task state:', result.state)
   "

3. Check Redis has the task queued:
   docker-compose exec redis redis-cli LLEN celery

4. Watch worker logs for task execution:
   docker-compose logs -f worker (for ~15 seconds to see progress updates)

5. Test cancellation flow works:
   docker-compose exec api python -c "
   from app.services.job_manager import request_cancellation, confirm_cancellation, is_job_cancelled
   request_cancellation('test123')
   print('Cancel pending:', is_job_cancelled('test123'))  # Should be False
   confirm_cancellation('test123')
   print('Cancel confirmed:', is_job_cancelled('test123'))  # Should be True
   "
  </action>
  <verify>
docker-compose down && docker-compose up --build -d
sleep 15
docker-compose logs worker 2>&1 | grep -E "(reconstruction.process|ready)"
docker-compose exec redis redis-cli PING
  </verify>
  <done>
Worker logs show task "reconstruction.process" registered. Task can be queued via delay(). Cancellation flags work in Redis. Services all running healthy.
  </done>
</task>

</tasks>

<verification>
- job_manager functions manipulate Redis correctly (request -> confirm -> is_cancelled flow)
- Celery task registers with correct name
- Task checks cancellation before each step
- Task reports progress via update_state with PROGRESS state
- Worker logs show task execution with progress updates
- Cancellation during task results in cleanup and REVOKED state
</verification>

<success_criteria>
1. Job manager implements two-step cancellation (request_cancellation -> confirm_cancellation)
2. is_job_cancelled returns True only after confirmation
3. Celery task uses shared_task decorator (not @celery.task)
4. Task updates progress at each step with percentage
5. Task checks cancellation before each step
6. Cancelled tasks delete their files and return cancelled status
7. Task completes full cycle in ~12 seconds with placeholder steps
</success_criteria>

<output>
After completion, create `.planning/phases/02-job-pipeline/02-03-SUMMARY.md`
</output>
