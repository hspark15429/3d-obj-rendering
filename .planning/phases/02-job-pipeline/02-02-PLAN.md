---
phase: 02-job-pipeline
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/api/__init__.py
  - app/api/schemas.py
  - app/services/__init__.py
  - app/services/file_handler.py
autonomous: true

must_haves:
  truths:
    - "Pydantic models validate job request/response formats"
    - "File handler validates PNG files by magic bytes"
    - "File handler rejects non-PNG and oversized files"
  artifacts:
    - path: "app/api/schemas.py"
      provides: "Request/response Pydantic models"
      exports: ["JobSubmitResponse", "JobStatusResponse", "CancelRequest", "CancelResponse"]
    - path: "app/services/file_handler.py"
      provides: "File validation and storage"
      exports: ["validate_upload_files", "save_job_files"]
  key_links:
    - from: "app/services/file_handler.py"
      to: "PNG magic bytes"
      via: "filetype.is_image check"
      pattern: "filetype"
    - from: "app/api/schemas.py"
      to: "JobStatusEnum"
      via: "status field validation"
      pattern: "queued.*processing.*completed.*failed.*cancelled"
---

<objective>
Create Pydantic schemas for API contracts and file handling service for upload validation.

Purpose: Type-safe API contracts and strict file validation ensure clean error handling and prevent invalid data from entering the pipeline. These are independent of Celery and can be developed in parallel.
Output: schemas.py with all job-related models, file_handler.py with PNG validation and storage logic.
</objective>

<execution_context>
@/home/devuser/.claude/get-shit-done/workflows/execute-plan.md
@/home/devuser/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-job-pipeline/02-CONTEXT.md
@.planning/phases/02-job-pipeline/02-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Pydantic schemas for job API</name>
  <files>app/api/__init__.py, app/api/schemas.py</files>
  <action>
Create app/api/__init__.py (empty, package marker).

Create app/api/schemas.py with:

1. JobStatus enum (str, Enum):
   - queued, processing, completed, failed, cancelled

2. JobSubmitResponse model:
   - job_id: str (8-char nanoid)
   - status: JobStatus (always "queued" on submit)
   - created_at: datetime (ISO 8601)

3. JobStatusResponse model:
   - job_id: str
   - status: JobStatus
   - progress: int | None (0-100, only when processing)
   - created_at: datetime
   - updated_at: datetime | None
   - error: str | None (only when failed)

4. CancelRequest model:
   - confirm: bool = False (two-step: first without confirm, then with confirm=true)

5. CancelResponse model:
   - job_id: str
   - status: Literal["cancel_requested", "cancelled"]
   - message: str

6. ErrorResponse model:
   - error: str
   - detail: str | None
   - fields: dict[str, str] | None (for field-level validation errors)

Use Pydantic v2 syntax (model_config instead of Config class).
Use datetime.utcnow() for timestamps - serializes to ISO 8601 automatically.
  </action>
  <verify>
python -c "
from app.api.schemas import JobSubmitResponse, JobStatusResponse, CancelRequest, CancelResponse, JobStatus
print('JobStatus values:', [s.value for s in JobStatus])
print('JobSubmitResponse fields:', JobSubmitResponse.model_fields.keys())
print('CancelRequest default confirm:', CancelRequest().confirm)
"
  </verify>
  <done>
All schema classes import without errors. JobStatus enum has 5 values. CancelRequest.confirm defaults to False. Models have correct field types.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create file handling service with PNG validation</name>
  <files>app/services/__init__.py, app/services/file_handler.py</files>
  <action>
Create app/services/__init__.py (empty, package marker).

Create app/services/file_handler.py with:

1. Constants:
   - EXPECTED_VIEWS = 6
   - EXPECTED_DEPTH = 6
   - MAX_FILE_SIZE = 20 * 1024 * 1024 (20MB per file - 2048x2048 PNGs can be large)
   - MAX_TOTAL_SIZE = 200 * 1024 * 1024 (200MB total)
   - PNG_MAGIC = b'\x89PNG\r\n\x1a\n'

2. FileValidationError(Exception) class:
   - message: str
   - field: str | None (which file/field failed)

3. async def validate_upload_files(views: list[UploadFile], depth_renders: list[UploadFile]) -> None:
   - Check views count == 6, raise FileValidationError if not
   - Check depth_renders count == 6, raise FileValidationError if not
   - For each file in both lists:
     - Read first 8 bytes, check PNG magic bytes
     - Seek to end, check size <= MAX_FILE_SIZE
     - Seek back to 0 (CRITICAL: reset file pointer after validation)
   - Check total size <= MAX_TOTAL_SIZE
   - Raise FileValidationError with specific message on any failure

4. async def save_job_files(job_id: str, views: list[UploadFile], depth_renders: list[UploadFile]) -> Path:
   - Create job directory: storage/jobs/{job_id}/
   - Create subdirs: views/, depth/
   - Use aiofiles for async file writing
   - Save views as views/view_{i:02d}.png (view_00.png to view_05.png)
   - Save depth as depth/depth_{i:02d}.png
   - Return Path to job directory

5. def get_job_path(job_id: str) -> Path:
   - Return Path to job directory (may not exist)

6. async def delete_job_files(job_id: str) -> bool:
   - Delete entire job directory if exists
   - Return True if deleted, False if didn't exist

Use Path from pathlib. Use aiofiles for all async I/O.
Import UploadFile from fastapi.
  </action>
  <verify>
python -c "
from app.services.file_handler import (
    validate_upload_files, save_job_files, delete_job_files,
    FileValidationError, EXPECTED_VIEWS, MAX_FILE_SIZE
)
print('Expected views:', EXPECTED_VIEWS)
print('Max file size:', MAX_FILE_SIZE)
print('FileValidationError imported:', FileValidationError.__name__)
"
  </verify>
  <done>
file_handler.py imports without errors. Constants are correct. FileValidationError is a proper exception class. All async functions are defined.
  </done>
</task>

<task type="auto">
  <name>Task 3: Write unit tests for file validation</name>
  <files>tests/__init__.py, tests/test_file_handler.py</files>
  <action>
Create tests/__init__.py (empty).

Create tests/test_file_handler.py with pytest tests:

1. test_validate_wrong_view_count:
   - Create mock UploadFiles (5 views instead of 6)
   - Assert raises FileValidationError

2. test_validate_wrong_depth_count:
   - Create mock UploadFiles (7 depth instead of 6)
   - Assert raises FileValidationError

3. test_validate_non_png_file:
   - Create mock UploadFile with JPEG magic bytes
   - Assert raises FileValidationError mentioning "PNG"

4. test_validate_oversized_file:
   - Create mock UploadFile larger than MAX_FILE_SIZE
   - Assert raises FileValidationError mentioning "size"

5. test_validate_success:
   - Create 6 valid PNG mock files for views and depth
   - Assert no exception raised

Use io.BytesIO for mock file content.
Use unittest.mock for UploadFile mocking (or create simple test fixtures).
Add pytest and pytest-asyncio to requirements.txt if not present.

NOTE: These tests run locally, not in Docker. They verify the validation logic.
  </action>
  <verify>
pip install pytest pytest-asyncio
python -m pytest tests/test_file_handler.py -v
  </verify>
  <done>
All 5 tests pass. File validation correctly rejects wrong counts, non-PNG files, and oversized files while accepting valid uploads.
  </done>
</task>

</tasks>

<verification>
- All schema classes import and have correct fields
- file_handler.py functions import without errors
- Unit tests pass for file validation edge cases
- PNG magic byte check works correctly
- File pointer is reset after validation (test verifies file can be read after validation)
</verification>

<success_criteria>
1. Pydantic schemas match API contract from CONTEXT.md (flat JSON, ISO 8601 timestamps)
2. JobStatus enum has all 5 states: queued, processing, completed, failed, cancelled
3. File handler validates PNG by magic bytes (not just extension)
4. File handler rejects wrong file counts with clear error messages
5. File handler rejects oversized files
6. All unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-job-pipeline/02-02-SUMMARY.md`
</output>
